Job Title:Developer

Location: Pune

Introduction

This role will be responsible for the development, and change & Release management of Ab-initio & Hadoop codes across various applications and/or scripts across Bigdata Platform, including owning Operational Acceptance Testing and Automation/Scheduling of Jobs (using TWS) of projects delivered on Hadoop environment.

What will you be doing?

The Role Holder Will Possess The Following

Manage client expectations and demands proactively while maintaining good client relationships, working closely with the Technical delivery Manager to ensure effective application development and delivery within the required project timescales and budget.
Utilise the Barclays defined best practices, standards and tools in application design, development, integration, documentation and support.
To establish strong relationships across the organisation and suppliers in order to develop effective applications and work in a fully collaborative way.
To leverage the use of agreed supply chain resources, ensuring that all third party resources are integrated into the Barclays application development team, with an agreed communication and reporting line to the AVP/VP.
To deliver an application, or enhance an application, that will each deliver benefits of strategic importance to a business area or group of business areas.
To deliver medium scale and relatively low-complexity change into business areas with a smooth transition into BAU operations, whilst ensuring agreed benefits are realised.
Responsible for the integration of new and modified applications onto the wider Barclays Systems landscape.
Be proficient in Ab-iniito and Hadoop Ecosystem & related open source tools – Hive, HBase, Spark, HDFS, Map Reduce, Pig etc..
Proficient in any of the Scheduling Tool (Woking knowledge of IBM Tivoli Identity Management will be an added advantage).
Provide guidance for Build in Implementation projects.
Peer reviews of team member's work.
To be an expert in required technical skills, open pick up new skills as mandated by the Barclaycard Data Technology Strategy.
Have technical ownership of Code migration, IBM Tivloi schedule build and manage team / deliverables of Barclay’s staff and 3rd party suppliers where applicable.
Deliver quality & high performance code, and be responsible to constantly improve the throughput of applications.


What We’re Looking For

Ab Initio, Informatica ETL tools
Bigdata technologies – HDFS, HIVE, MongoDB, Scoop, Spark, Scala
The person should have strong knowledge in SQL and UNIX


Skills That Will Help You In The Role

Sufficient Cards business knowledge/experience in order to interact productively with business
Risk and issue management techniques and experience
At least 3 years’ experience in of working within technology in business domain

Where will you be working?

Pune














Job Title:Developer

Location: Pune

Introduction

Design & Implement Data integration and Data Warehouse based solutions using Hadoop / Big data solutions. The Tech Lead should be highly proficient in the use of Hadoop and standard techniques of Data Integration, Data Manipulation. Translate business requirements & E2E designs into technical implementations based on system capabilities. Need to define optimal solutions addressing functional and non- functional requirements taking into account capabilities across BARCLAYCARD DATA. Ensure the solutions being defined are reusable and follow standard design patterns and architecture principles of Hadoop. Work closely with the Portfolio Design Lead and Portfolio Delivery Lead / ADM to define deliverables and plans.

Lead on Defect resolution and help the development teams with technical expertise. Provide guidance in Live Issue resolution and RCA Reviews with the development teams. The designer should be proactive in working with the support team and identifying opportunities for service improvement.

The role will require to work across a spectrum of requirements across different banking functions

What will you be doing?

As a member of engineering team person will continue to build, extend, innovative next generation banking solutions and capabilities.
This engineer will be responsible for code delivery in a timely and accurate fashion, and the overall review and guidance of sprint and epic goals.
This engineer is expected to produce a high volume of quality code. The leads to work specific stories and contribute to sprint panning, and feature identification will task them.
Good team player
Solution walkthroughs with development and test team. Handhold development team in the implementation of technical solutions, if required.


What We’re Looking For

Experience in Hadoop, Hive, Spark, Spark Streaming, Kafka, MongoDB, HBase, Cassandra
UNIX experience including shell scripting. Cluster Management, Kerberos for security
Provide expertise and continuity across both the business solution and technical solution involving Ab Initio Build effective relationships with all stakeholders including federated project teams, BU SPOC, GIS etc
Core Programming including but not limited to Java, Scala, Bash, C / C++


Skills That Will Help You In The Role

B. Tech. (or equivalent) / MCA with good technology skills
Detailed and working understanding of DWH principals and implementation.
Good understanding of AWS / Machine learning. (added advantage)


Where will you be working?

Pune
















Job Title:ETL Developer

Location: Pune

This role spans across leading / working on multiple projects or modules and coordinating with teams to understand requirements, develop and implement AbInitio based ETL solutions and maintaining them.

What will you be doing?

Design & Development

Delivery of high level and low level design documents with middleware strategy, standards and adhering to CDS middleware principles & standards
Leading a team of developers working on one or multiple projects of varying complexity, undertaking the following activities, maintaining overall accountability for the work of the team in terms of quality and timeliness of delivery:
Strong understanding of Middleware Enterprise designs with functional and non-functional assessment for global implementations
Design and design review of dependent programs
Strong technical problem solving capabilities
Supporting and coaching
Issue escalation
Validation of plans and progress management
QA of deliverables


What We’re Looking For

Strong knowledge of ETL dependent technologies in the below scope:

AbInitio Co>Op 2.X/3.0/3.2/3.3 /
AbInitio Conduct>It
A>I Enterprise Meta Environment
SQL
Oracle (10g/11g/12c) & Teradata
Unix basics and shell scripting
Functional Skills/Competencies:
Requirement gathering & Analysis and other SDLC phases
Coding with middleware principles
4 to 8 years of experience in ETL development and implementation
Well versed with ETL concepts and performance tuning
Good written and verbal communication skills
Team / module leading experience
Hands on AbInitio development with design and build of medium to large enterprise projects
Maintaining and demonstrating a good knowledge of the Financial Services and IT industries, having an in depth knowledge of either several technologies/ systems/ business operations, or expert knowledge of a particular subject area
Working on a single complex project, or multiple standard projects, as a development lead
Leading a team of developers working across a number of simple or moderately complex projects, or on a large complex project
Understanding the solution design and any associated architectural standards, and supporting the resolution of design/build related issues throughout the project delivery lifecycle
Supporting project management in planning, estimating, costing, RAID identification and management
Assisting in the definition and maintenance of middleware implementation level architectures
Experience in dealing with various file formats like XML, COBOL EBCDIC and flat files


Skills That Will Help You In The Role

Basic data modelling knowledge
Strong skills on ETL design patterns and integration architecture for Middleware design and implementation
Extensively worked with Business Analysts to convert requirements to high and low level design for ETL programs (middleware is preferred)


Where will you be working?

Pune










Job Title:Sr Abinitio Developer

Location: Pune

This role will support Financial Crime Data and Analytics team efforts in building and implementing a data lake which will promote and facilitate a data-led approach for the identification and disruption of financial crime activities.

Serves as SME for AbInitio product and align his/her understanding with overall purpose of the project and contribute in design, development, testing and maintenance on Big Data Platform
Supports the Project Manager as well as Architect Owners in build frameworks and technical design and develop code with clear scope and acceptance criteria in line with Agile delivery methodology
Act as a Technical Leads and generate flow diagrams, tables, mock-ups and other methods to ensure features and stories are presented without ambiguity when designing the solution.
Technical ownership of code delivery and ensuring code quality is maintained.
Involve in test phases and work towards early resolution of defects and ensuring permanent fix
Effective liaison with Infrastructure, Hadoop admin and other engineering teams
Technical Design, Develop and Implement Change Requests (CR) for Business
Adhere to Agile methodology, ensure requirements documentation complies with Agile and audit standards.
Adhere to all Barclays, Group centre and compliance technology standards, policies and governance practices.
Contributes to and use best practices and standards including Barclays Values & Behaviours.


What will you be doing?

As a Sr. developer, possess excellent Knowledge of Ab Initio stack and its implementation in Big Data Space(Data Lake)
Technical Design and development of ETL/Hadoop and Analytics services /components
Contribute in end to end architecture and process flow
Understand Business requirement and publish reusable designs
Result oriented approach with ability to provide apt solutions.
Proficient in performance improvement & fine-tuning ETL and Hadoop implementations
Conduct code reviews across projects. Takes responsibility for ensuring that build and code adhere to architectural and quality standards and policies.
Can work independently with minimum supervision.
Strong analytical and problem solving skills
Experience/Exposure to SQL, advanced SQL skills


What We’re Looking For

Bachelor’s Degree
Relevant years of Hands-on experience with Ab Initio and Hadoop technologies (HDFS, HIVE, Impala, Scala, Spark, PIG etc)
Experience in Relational Databases like Oracle, SQL Server and PL/SQL
Understanding of Agile methodologies as well as SDLC life-cycles and processes.
Expertise in ETL technology (Ab Initio, Hadoop)
Expertise in UNIX scripts, DB & TWS.
Familiarity with Tableau, Python or R is a plus
Strong Understanding of Data warehousing and lakes


Skills That Will Help You In The Role

Very Good communication and organizational skills
Very Good analytical skills
Promote Automation, reusable components
Financial Crime or Compliance experience is preferred
Ability to understand business requirements and design software solutions.
Strong data analysis skills and ability to present the findings to the business users.
Quick learner; strong analytical and problem-solving skills, with the ability to deal with numerous tasks simultaneously and with frequently changing priorities.
Financial background is highly preferred.
Demonstrable ability to communicate and build relationships with members of the business and technology communities.
Strong written and verbal communication skills

Must be independent and creative in approach to problems and issues; assertive, tenacious, proactive


Where will you be working?

Pune

